{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drift-diffusion models (DDMs)\n",
    "\n",
    "_Kira Dusterwald, November 2022, based loosely on https://compneuro.neuromatch.io/tutorials/W2D2_LinearSystems/student/W2D2_Tutorial3.html and https://gist.github.com/mwaskom/f8c118a956c549ecea7c. There are some great explanations and some derivations in https://academic.oup.com/mit-press-scholarship-online/book/29407/chapter/244818296 (also available as a textbook in Gatsby!)._\n",
    "\n",
    "DDMs are common perceptual decision-making models pioneered by Ratcliff. They have been used to explain the mechanisms of (2-alternative forced choice*) decision-making, and to account for outcomes like direction of choice and reaction time. \n",
    "\n",
    "In short, DDMs model the accumulation of evidence towards bounds over time in an uncertain environment.\n",
    "\n",
    "The purpose of this notebook / assignment is to provide a general framework for concepts you will encounter in the perceptual decision-making neuroscience literature, and an intuition of how to implement a drift-diffusion model yourself.\n",
    "\n",
    "*at least for the simple versions we discuss here, but there are ways to extend the concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting helper function\n",
    "\n",
    "def plotting(trials,decthreshold=10):\n",
    "    f, ax = plt.subplots()\n",
    "    if hasattr(trials[0], '__len__'):\n",
    "        for t in trials:\n",
    "            ax.plot(t,alpha=0.5)\n",
    "    else:\n",
    "        ax.plot(trials,alpha=0.5)\n",
    "    ax.axhline(0, c=\".7\")\n",
    "    ax.axhline(decthreshold, c=\".2\")\n",
    "    ax.axhline(-decthreshold, c=\".2\")\n",
    "    ax.set(ylim=(-decthreshold - 2, decthreshold + 2))\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel('evidence')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal detection theory (SDT)\n",
    "\n",
    "SDT has historically been employed to understand decision-making given uncertainty, literally detecting the \"signal\" from the \"noise\", and it's where we start our foray into decision-making models.\n",
    "\n",
    "Consider a scenario in which a doctor needs to look at a chest x-ray and comment on whether or not the patient has tuberculosis (TB). There are many variables that affect x-ray quality and the manifestation of disease on an x-ray, which provide _uncertainty_. The doctor answers, producing several outcomes, which I include because you'll likely see them used in papers:\n",
    "* True hit (true positive, i.e. has TB and x-ray = TB)\n",
    "* False alarm (false positive, i.e. does not have TB but x-ray = TB)\n",
    "* Correct rejection (true negative, i.e. does not have TB and x-ray $\\neq$ TB)\n",
    "* Miss (false negative, i.e. has TB but x-ray $\\neq$ TB)\n",
    "\n",
    "Perceptually, we separate this process into an information acquisition stage, which can be corrupted by **external noise** like imaging quality, and a criterion or response stage, which might be corrupted by a doctor's **prior** or bias. For example, if a doctor believes that TB is dangerous and prevalent, they might think it is better to start treatment, and so err on diagnosing TB in conditions of uncertain information.\n",
    "\n",
    "The information acquisition part is sometimes called a \"generative process\" since it generates an **internal representation** of the variable of interest in the x-ray in terms of neural spikes. This, of course, is also party to **internal noise** because neurons do not provide perfect fidelity. We assume that while an individual neuron might not code the doctor's decision about TB, there is some **internal response** generated from the internal representation, i.e. an aspect of neural activity that reflects the doctor's decision.\n",
    "\n",
    "We will make more of these concepts concrete as we go along, but the terminology is useful to take on.\n",
    "\n",
    "**PROBLEM**\n",
    "1. You could imagine a fun test between doctors when we ask them to give a diagnosis as quickly as possible. Then the doctor will make some rushed decisions. What might happen to accuracy (i.e. proportion of correct responses (true hits + correct rejections)) in this scenario compared to on a normal day?\n",
    "\n",
    "2. Can SDT account for the accuracy-response time trade off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Time and diffusion\n",
    "\n",
    "What's significantly left out in the SDT framework is an incorporation of a key variable: time. This matters for several reasons. First, when some variable is loaded in working memory, there should be some lag between stimulus and response of the subject. Noise over time doesn't stay where it is, and it's important to incorporate how it moves, or diffuses. \n",
    "\n",
    "As an example, let's simulate a random walk (Brownian motion). Say you are watching a particle that starts at a position $x=0$. \n",
    "\n",
    "**PROBLEM** \n",
    "\n",
    "3. First, at each time step, simulate and plot over time what happens when your particle goes up or down $x$ by 1 with equal probability (like a coin flip). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time array and position array\n",
    "t_end = 100\n",
    "times = np.arange(t_end)\n",
    "x = [0]\n",
    "t = 1\n",
    "\n",
    "# loop through time\n",
    "while t < t_end:\n",
    "    evidence = ## YOUR CODE HERE\n",
    "    x.append() ## YOUR CODE HERE\n",
    "    t += 1\n",
    "\n",
    "plotting(np.array(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**\n",
    "\n",
    "4. Now subsitute your rule of how the position shifts at each time step for a Gaussian random variable (start with mean $\\mu = 0$ and variance $\\sigma^2 = 1$, that is $x \\sim \\mathcal{N}(0,1)$). What happens when you change the mean and variance?\n",
    "\n",
    "5. Simulate multiple trajectories. What happens when you change the time horizon?\n",
    "\n",
    "6. What is the expected value of the particle's position as a function of time? What is the empiric average? What is the expected variance as a function of time? Plot the empiric mean and variance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_walk(mean,variance):\n",
    "    \n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement for multiple trials\n",
    "ntrials = 1000\n",
    "mymean = 0\n",
    "myvar = 1\n",
    "\n",
    "array_trials = np.array([rand_walk(mymean,myvar) for _ in range(ntrials)])\n",
    "plotting(array_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empiric_mu = ## YOUR CODE HERE\n",
    "empiric_var = ## YOUR CODE HERE\n",
    "\n",
    "plt.plot(empiric_mu)\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.xlabel('time')\n",
    "plt.ylim([-1-mymean,1+mymean])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(empiric_var)\n",
    "plt.ylabel('$\\sigma^2$')\n",
    "plt.xlabel('time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS**\n",
    "\n",
    "7. Derive the expected mean and variance after $\\tau$ time steps, given $\\mu$ and $\\sigma^2$ fixed for each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a bound to signal evidence accumulation\n",
    "\n",
    "**PROBLEM**\n",
    "\n",
    "8. Add _decision bounds_ to your code by copying the single trial function to the cell below. I.e. When an upwards or downwards random walk gets to a set bound, terminate that trajectory. Then plot the reaction times as a histogram. What does the distribution of the RT histogram look like? Play with the parameters a little to get a feel for how diffusion affects RTs.\n",
    "\n",
    "9. How would you calculate the proportion of trials terminating at the \"top\" and \"bottom\" bounds respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_walk_bound(mean,variance,bound):\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    return np.array(x)\n",
    "\n",
    "# implement for multiple trials\n",
    "ntrials = 1000\n",
    "mymean = 0\n",
    "myvar = 2\n",
    "mybound = 10\n",
    "\n",
    "array_trials_bound = np.array([rand_walk_bound(mymean,myvar,mybound) for _ in range(ntrials)])\n",
    "plotting(array_trials_bound,decthreshold=mybound)\n",
    "\n",
    "# calculate reaction times\n",
    "reaction_times =  ## YOUR CODE HERE\n",
    "\n",
    "plt.hist(reaction_times)\n",
    "plt.xlabel('time bins')\n",
    "plt.ylabel('proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate proportions\n",
    "prop_top =  ## YOUR CODE HERE\n",
    "prop_bottom =  ## YOUR CODE HERE\n",
    "\n",
    "print('proportion of top bound: ' + str(np.round(prop_top,2)))\n",
    "print('proportion of bottom bound: ' + str(np.round(prop_bottom,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this link to DDM?\n",
    "\n",
    "In a DDM sense, momentary evidence is corrupted by noise which is integrated over time. This causes the \"decision variable\" to move around in a Brownian way, until it hits or gets \"absorbed\" by some bound, at which point that bound wins and the decision is made. The time this takes gives one the reaction time. The amount of noise (variance) might influence when a decision is made or not.\n",
    "\n",
    "Using diffusion like this is a popular way to model working memory: hanging onto a variable imperfectly.\n",
    "\n",
    "We would like to include some deterministic _bias_ in our model of decision-making, which can help us weight decisions but decays over time. To do so, incorporate a *drift* term: a term that slowly tends towards some value. This you can think of in the dynamical systems sense, so that sensory information from previous time steps is weighted less over time.\n",
    "\n",
    "We can think of a simple linear differential equation in continuous time as $\\frac{d}{dt} x =  (\\lambda-1)(x - x_\\infty)$ for some scalar $\\lambda \\in \\mathbb{R}$ and constant $x_\\infty$. However so that we can simulate it, we discretise it*:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{t+1} = \\lambda (x_t - x_\\infty) + x_\\infty\n",
    "\\end{equation*}\n",
    "\n",
    "**PROBLEM**\n",
    "\n",
    "10. Solve this DE. Assume $x_0$ is the value of $x$ when $t=0$.\n",
    "\n",
    "11. Simulate this DE using either the closed form solution, discretised approach, or a numerical approach (e.g. Euler's; speak to a TA if you have not heard of this before!).\n",
    "\n",
    "12. How does the value of $\\lambda$ affect the solution? When is it stable or not?\n",
    "\n",
    "*Slightly informal, but basically this is a Euler approach with $dt=1$. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lam = 0.9 # e^(lambda-1)\n",
    "x_0 = 2\n",
    "x_infinity = 1\n",
    "t_end = 100\n",
    "times = np.linspace(0,t_end,1000)\n",
    "\n",
    "x = ## YOUR CODE HERE\n",
    "\n",
    "plt.plot(times, x)\n",
    "plt.xlabel('time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full DDM!\n",
    "\n",
    "The full DDM is just a combination of the drift and diffusion terms, that is:\n",
    "\\begin{equation*}\n",
    "x_{t+1} = \\lambda (x_t - x_\\infty) + x_\\infty + \\kappa \\xi\n",
    "\\end{equation*},\n",
    "\n",
    "where $\\kappa$ can be set and $\\xi \\sim \\mathcal{N} (\\mu,\\sigma)$. Usually, $\\mu =0$.\n",
    "\n",
    "**PROBLEM**\n",
    "\n",
    "13. Implement full DDM. Simulate several trials and explore.\n",
    "\n",
    "14. What happens to the empirical mean and variance in full DDM over time? Plot this. Play with the parameters. What does the variance seem to depend on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.9\n",
    "x_inf = 1\n",
    "t_end = 100\n",
    "trials = 100\n",
    "\n",
    "## YOUR CODE HERE\n",
    "array_trials = ##\n",
    "\n",
    "plotting(array_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, you should see that the inclusion of the deterministic term provides a nice hack to balance the continually growing variance from the stochastic diffusion process.\n",
    "\n",
    "**BONUS**\n",
    "\n",
    "15. Calculate the analytical variance of the full DDM at equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting behavioural data\n",
    "\n",
    "So we've now walked through the model, but we would also like to be able to apply it to real data. There are a few general approaches for this type of model-fitting exercise, but these basically boil down to simulating the DDM many times with different parameter values until you optimise a criterion (e.g. maximising the likelihood of the data). You can employ a non-linear search algorithm to efficiently work through the parameter space. This type of approach may be needed for complicated models or when you really want to fit some more nuanced dynamic, e.g. the reaction time distribution. \n",
    "\n",
    "However, for a simple fit of the accuracy and mean reaction times, there is a closed form solution! Let's assume we just have the diffusion part for this exercise, i.e. $x_{t+1} = x_t + \\xi$, and let the top bound be given by $\\beta$ and the bottom by $-\\beta$. This is still an okay model for working memory, we just do not have the decay term for the sensory evidence mean. Then we can show that:\n",
    "\n",
    "\\begin{equation*}\n",
    "P_{top} = \\frac{1}{1+\\exp\\Big[-\\frac{2\\mu \\beta}{\\sigma^2}\\Big]}\n",
    "\\end{equation*}\n",
    "\n",
    "**BONUS**\n",
    "\n",
    "16. Derive this expression.\n",
    "\n",
    "17. Simulate the equation for different values of $\\mu$. What about the bounds and the variance? (see code below)\n",
    "\n",
    "18. Derive the expression to calculate the mean reaction time for when you hit the bound,\n",
    "\n",
    "\\begin{equation*}\n",
    "RT_{mean} = \\frac{\\beta}{\\mu}\\tanh[\\mu\\beta]\n",
    "\\end{equation*}\n",
    "\n",
    "19. Plot the expected mean RT for different values of $\\mu$. What happens to this expression when $\\mu \\rightarrow 0$? Take the limit and derive the solution. You can think of this as a case for which there is no sensory evidence, so it is important to cover.\n",
    "\n",
    "20. What happens when $\\mu$ is really large? RTs cannot be instantaneous! To fix this problem, people add or fit a fixed delay constant term $\\tau$ to the mean RT equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q 17\n",
    "beta = 1\n",
    "mu = np.linspace(0, 10, 100)\n",
    "var = 1\n",
    "p = ## YOUR CODE HERE\n",
    "\n",
    "f, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(mu, p)\n",
    "ax.set(xlabel=\"$\\mu$\", ylabel=\"$P_{correct}$\", ylim=(.5, 1.1))\n",
    "\n",
    "# q 19\n",
    "rt = ## YOUR CODE HERE\n",
    "f, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(mu, rt)\n",
    "ax.set(xlabel=\"$\\mu$\", ylabel=\"$RT_{mean}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting behavioural data continued\n",
    "\n",
    "We can use the framework above to fit data from a motion coherence task: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=8257671&retmode=ref&cmd=prlinks. \n",
    "\n",
    "This is left as an exercise to the reader: use the parameters in Fig 3. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
